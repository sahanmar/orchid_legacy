
% ================================================
\section{Motivation}\label{sec:motication}

Modern Natural Language Processing (NLP) approaches are able to achieve significant results in standard textual analysis tasks. 
The list of tasks includes but is not limited to such fields as text classification, e.g. determining the general topic of the news article \cite{text-classification-Altinel2018} or determining text author's attitude towards the topic \cite{sentiment-analysis-Medhat2014}; sequence tagging,  e.g. named entity recognition (NER)  \cite{ner-Strakova2019,ner-Zhanming2019,ner-Yamada2020,ner-Luoma2020} and part-of-speech tagging \cite{pos-tagging-Bohnet2018}; and text generation \cite{text-gen-Guo2017}. For some applications it is important to combine these tasks to achieve more comprehensible results. 
For instance, in the general case of the sentiment analysis task one aims to classify  whether the author of the piece of text refers to the topic in a negative or positive sense. However, to obtain a finer understanding of why their attitude is inferred to be some particular value, it is important to discern contextual dependencies within the piece, especially in cases when the range of output values goes beyond “polarity” (positive/neutral/negative) and matures into a broader spectrum of values like doubt, contempt, or enjoyment. 
The NLP research community not only actively develops models for improved natural language understanding, i.e. representing language in a vector space, \cite{gpt-Radford2018,bert-Devlin2019,xlnet-Yang2020} but also proposes different fine-tuning approaches for these models \cite{robarta-Liu2019,spanbert-Joshi2020,gpt2-Radford2019,gpt3-Brown2020}. 

In this project we aim to perform research in construction of better textual dependencies for the task in the form of improved Coreference Resolution (CR). The CR task is very complex to solve and from 2019 to this date \cite{spanbert-Joshi2020} no improvement was achieved on the standard benchmark \cite{ontonotes5-Weischedel2013}. The recent article \cite{cr-Toshniwal2020} achieved the same metrics as the one from 2019. Coreference resolution combines detection and linking of various mentions of entities within the text: linking noun phrases with their counterparts and pronouns, anaphora disambiguation, linking words with their pro-forms, etc.  These models have a significant impact on the quality of the text mining algorithms. A good use case where  coreference resolution can be applied is categorization of entities and their pronouns in order to provide one with a wider spectrum of information for future decision making. Based on the extracted data it is possible to unify all knowledge in a form of a Knowledge Graph (KG). The dependencies and connections between the entities can be used for enriching the feature space with the highly discriminative samples for the further tasks. 
For example, let us assume that we have two consecutive sentences like “John Smith and Amanda Brown are accountants in XYZ company. Amanda’s colleague was accused of drunk driving”. Based on these sentences we would like to classify if some of the entities from the text can be charged for a misdemeanor. For a human reader it is obvious that Amandas’s colleague refers to John. However for a machine that is a very hard task. However, proper identification of entity clusters like {John Smith, Amanda’s colleague}, {Amanda Brown}, {XYZ} would greatly improve the machine’s understanding of the piece of text.

In the scope of this work, we expect to improve the current state of the art (see the following section) by means of its further augmentation. Firstly, we believe that advancement can be achieved via the modification of the existing CR-solving model which is applied on top of vector embeddings. Since the model relies on scoring entity mentions and clustering them, significant changes can be brought with nonlinear dimensionality reduction which, in neural-network-based structures, can be achieved by means of autoencoders \cite{autoencoders-Zabalza2016,autoencoders-Sahay2019}. Moreover, advancements from the named entity recognition field may also provide us with meaningful results, as NER models also focus on entities and context surrounding them: in this case conditional random fields \cite{ner-Strakova2019,ner-Zhanming2019} are a potential candidate. In addition to that, we wish to entertain the possibility of integration of uncertainty [], as it has been shown that this approach may enhance the learning process significantly.

As another output of this project we expect to not only push the boundaries of the state-of-the-art algorithms but also introduce the first Czech and Slovak Coreference Resolution dataset that can be used as a new benchmark for the Czech and Slovak CR models evaluation. Based on the new data and better Coreference Resolution approach  we would like to generalize the CR algorithm as a Multilingual solution.


% ================================================
\section{State of the Art}\label{sec:sota}

Modern Coreference Resolution (CR) algorithms are combinations of sophisticated vector embeddings representing context and deep neural network superstructures that perform the coreference resolution itself. 

Natural language understanding (NLU) models. 
The set of existing models for NLU is vast. 
Arguably, one of the most prominent points in history of such models is when the continuous bag-of-words and skip-gram approaches were introduced \cite{w2v-Mikolov2013}. 
At that point machines started to be able to learn the context surrounding particular words and their vector representations  acquired the ability to represent this context, meaning proximity of such vectors in terms of a metric of choice (L2, cosine/angular similarity) veritably described similarity of words or contexts. 
Still, models of these types were far from perfect, as they provided one with constant vectors per word for a pre-set vocabulary. 
Context-dependent representations with flexible vocabularies became available thanks to the introduction of the Transformer architecture \cite{transformer-Vaswani2017} applied on the vocabulary formed not only by words but also by character n-grams constructed as meaningful parts of words.  
The power of the Transformer architecture lies in its encoding and decoding capability improved by the self-attention mechanism which learns to put stress on parts of text sequences. 
This gave birth to a lot of transformer-based language models such as the Bidirectional Encoder Representations from Transformers (BERT) \cite{bert-Devlin2019}, its fine-tuned variations \cite{albert-Lan2020,robarta-Liu2019} and further models \cite{gpt-Radford2018,use-Cer2018}. 
To this date, SpanBERT \cite{spanbert-Joshi2020} has proven to be the most efficient architecture for coreference resolution. 
Its crucial difference from the standard BERT model is that it learns to predict the content of masked spans of text, taking into account their beginnings and endings, omitting the ability of the base BERT model to predict foregoing  sentences, whereas BERT learns to predict the following sentence for each preceding one and attempts to infer individual masked tokens. 