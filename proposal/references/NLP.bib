% Text Classification
@article{text-classification-Altinel2018,
title = {Semantic text classification: A survey of past and recent advances},
journal = {Information Processing \& Management},
volume = {54},
number = {6},
pages = {1129-1153},
year = {2018},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0306457317305757},
author = {B. Altinel, and M. C. Ganiz},
keywords = {Text classification, Semantic text classification, Knowledge-based systems, Corpus-based systems, Neural language models, Deep learning},
abstract = {Automatic text classification is the task of organizing documents into pre-determined classes, generally using machine learning algorithms. Generally speaking, it is one of the most important methods to organize and make use of the gigantic amounts of information that exist in unstructured textual format. Text classification is a widely studied research area of language processing and text mining. In traditional text classification, a document is represented as a bag of words where the words in other words terms are cut from their finer context i.e. their location in a sentence or in a document. Only the broader context of document is used with some type of term frequency information in the vector space. Consequently, semantics of words that can be inferred from the finer context of its location in a sentence and its relations with neighboring words are usually ignored. However, meaning of words, semantic connections between words, documents and even classes are obviously important since methods that capture semantics generally reach better classification performances. Several surveys have been published to analyze diverse approaches for the traditional text classification methods. Most of these surveys cover application of different semantic term relatedness methods in text classification up to a certain degree. However, they do not specifically target semantic text classification algorithms and their advantages over the traditional text classification. In order to fill this gap, we undertake a comprehensive discussion of semantic text classification vs. traditional text classification. This survey explores the past and recent advancements in semantic text classification and attempts to organize existing approaches under five fundamental categories; domain knowledge-based approaches, corpus-based approaches, deep learning based approaches, word/character sequence enhanced approaches and linguistic enriched approaches. Furthermore, this survey highlights the advantages of semantic text classification algorithms over the traditional text classification algorithms.}
}

% Sentiment Analysis
@article{sentiment-analysis-Medhat2014,
title = {Sentiment analysis algorithms and applications: A survey},
journal = {Ain Shams Engineering Journal},
volume = {5},
number = {4},
pages = {1093-1113},
year = {2014},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2014.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S2090447914000550},
author = {W. Medhat and A. Hassan and H. Korashy},
keywords = {Sentiment analysis, Sentiment classification, Feature selection, Emotion detection, Transfer learning, Building resources},
abstract = {Sentiment Analysis (SA) is an ongoing field of research in text mining field. SA is the computational treatment of opinions, sentiments and subjectivity of text. This survey paper tackles a comprehensive overview of the last update in this field. Many recently proposed algorithms' enhancements and various SA applications are investigated and presented briefly in this survey. These articles are categorized according to their contributions in the various SA techniques. The related fields to SA (transfer learning, emotion detection, and building resources) that attracted researchers recently are discussed. The main target of this survey is to give nearly full image of SA techniques and the related fields with brief details. The main contributions of this paper include the sophisticated categorizations of a large number of recent articles and the illustration of the recent trend of research in the sentiment analysis and its related areas.}
}

% POS Tagging
@inproceedings{pos-tagging-Bohnet2018,
    title = "Morphosyntactic Tagging with a Meta-{B}i{LSTM} Model over Context Sensitive Token Encodings",
    author = "Bohnet, B.  and
      McDonald, R.  and
      Sim{\~o}es, G.  and
      Andor, D.  and
      Pitler, E.  and
      Maynez, J.",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1246",
    doi = "10.18653/v1/P18-1246",
    pages = "2642--2652",
    abstract = "The rise of neural networks, and particularly recurrent neural networks, has produced significant advances in part-of-speech tagging accuracy. One characteristic common among these models is the presence of rich initial word encodings. These encodings typically are composed of a recurrent character-based representation with dynamically and pre-trained word embeddings. However, these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts. In this paper, we investigate models that use recurrent neural networks with sentence-level context for initial character and word-based representations. In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states.",
}

% NER
@misc{ner-Zhanming2019,
      title={Dependency-Guided LSTM-CRF for Named Entity Recognition},
      author={Zhanming, J. and Wei, L.},
      year={2019},
      eprint={1909.10148},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ner-Yamada2020,
      title={LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention},
      author={I. Yamada and A. Asai and H. Shindo and H. Takeda and Y. Matsumoto},
      year={2020},
      eprint={2010.01057},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ner-Luoma2020,
      title={Exploring Cross-sentence Contexts for Named Entity Recognition with BERT},
      author={J. Luoma and S. Pyysalo},
      year={2020},
      eprint={2006.01563},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ner-Strakova2019,
      title={Neural Architectures for Nested NER through Linearization},
      author={J. Straková and M. Straka and J. Hajič},
      year={2019},
      eprint={1908.06926},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


% Text Generation
@misc{text-gen-Guo2017,
      title={Long Text Generation via Adversarial Training with Leaked Information},
      author={J. Guo and S. Lu and H. Cai and W. Zhang and Y. Yu and J. Wang},
      year={2017},
      eprint={1709.08624},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% BERT
@misc{bert-Devlin2019,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
      author={J. Devlin and M.-W. Chang and K. Lee and K. Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{robarta-Liu2019,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
      author={Y. Liu and M. Ott and N. Goyal and J. Du and M. Joshi and D. Chen and O. Levy and M. Lewis and L. Zettlemoyer and V. Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{albert-Lan2020,
      title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
      author={Z. Lan and M. Chen and S. Goodman and K. Gimpel and P. Sharma and R. Soricut},
      year={2020},
      eprint={1909.11942},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% XLNet
@misc{xlnet-Yang2020,
      title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
      author={Z. Yang and Z. Dai and Y. Yang and J. Carbonell and R. Salakhutdinov and Q. V. Le},
      year={2020},
      eprint={1906.08237},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% GPT
@inproceedings{gpt-Radford2018,
  title={Improving Language Understanding by Generative Pre-Training},
  author={A. Radford and K. Narasimhan},
  year={2018}
}

@inproceedings{gpt2-Radford2019,
  title={Language Models are Unsupervised Multitask Learners},
  author={A. Radford and J. Wu and R. Child and D. Luan and D. Amodei and I. Sutskever},
  year={2019}
}

@misc{gpt3-Brown2020,
      title={Language Models are Few-Shot Learners},
      author={T. B. Brown and B. Mann and N. Ryder and M. Subbiah and J. Kaplan and P. Dhariwal and A. Neelakantan and P. Shyam and G. Sastry and A. Askell and S. Agarwal and A. Herbert-Voss and G. Krueger and T. Henighan and R. Child and A. Ramesh and D. M. Ziegler and J. Wu and C. Winter and C. Hesse and M. Chen and E. Sigler and M. Litwin and S. Gray and B. Chess and J. Clark and C. Berner and S. McCandlish and A. Radford and I. Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Coreference Resolution
@misc{cr-Toshniwal2020,
      title={Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks},
      author={S. Toshniwal and S. Wiseman and A. Ettinger and K. Livescu and K. Gimpel},
      year={2020},
      eprint={2010.02807},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{cr-Lee17,
  author    = {K. Lee and
               L. He and
               M. Lewis and
               L. Zettlemoyer},
  title     = {End-to-end Neural Coreference Resolution},
  journal   = {CoRR},
  volume    = {abs/1707.07045},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.07045},
  archivePrefix = {arXiv},
  eprint    = {1707.07045},
  timestamp = {Mon, 13 Aug 2018 16:48:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LeeHLZ17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cr-Joshi2019,
  author    = {M. Joshi and
               D. Chen and
               Y. Liu and
               D. S. Weld and
               L. Zettlemoyer and
               O. Levy},
  title     = {SpanBERT: Improving Pre-training by Representing and Predicting Spans},
  journal   = {CoRR},
  volume    = {abs/1907.10529},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.10529},
  archivePrefix = {arXiv},
  eprint    = {1907.10529},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-10529.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{cr-Xia2020,
      title={Incremental Neural Coreference Resolution in Constant Memory},
      author={P. Xia and J. Sedoc and B. Van Durme},
      year={2020},
      eprint={2005.00128},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Universal Sentence Encoder
@misc{use-Cer2018,
      title={Universal Sentence Encoder},
      author={D. Cer and Y. Yang and S. Kong and N. Hua and N. Limtiaco and R. St. John and N. Constant and M. Guajardo-Cespedes and S. Yuan and C. Tar and Y.-H. Sung and B. Strope and R. Kurzweil},
      year={2018},
      eprint={1803.11175},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Word2Vec
@misc{w2v-Mikolov2013,
      title={Efficient Estimation of Word Representations in Vector Space},
      author={T. Mikolov and K. Chen and G. Corrado and J. Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Transformer
@misc{transformer-Vaswani2017,
      title={Attention Is All You Need},
      author={A. Vaswani and N. Shazeer and N. Parmar and J. Uszkoreit and L. Jones and A. N. Gomez and L. Kaiser and I. Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Translation
@article{translation-Bahdanau2014,
title = "Neural machine translation by jointly learning to align and translate",
author = "Bahdanau, D. and Cho, K. and Bengio, Y.",
year = "2014",
language = "English (US)",
journal = "arXiv",
}